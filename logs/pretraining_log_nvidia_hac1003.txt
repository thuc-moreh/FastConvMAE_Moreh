The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
Not using distributed mode
[08:25:52.225891] job dir: /nas/thuchk/FastConvMAE
[08:25:52.225985] Namespace(accum_iter=1,
batch_size=64,
blr=0.001,
data_path='/nas/common_data/imagenet_tiny',
device='cuda',
dist_on_itp=False,
dist_url='env://',
distributed=False,
epochs=10,
input_size=224,
local_rank=-1,
log_dir='./output_dir',
lr=None,
mask_ratio=0.75,
min_lr=0.0,
model='fastconvmae_convvit_base_patch16',
norm_pix_loss=False,
num_workers=10,
output_dir='./output_dir',
pin_mem=True,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.05,
world_size=1)
[08:25:52.232649] Dataset ImageFolder
    Number of datapoints: 1300
    Root location: /nas/common_data/imagenet_tiny/train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
           )
[08:25:52.232793] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fd40af5ad90>
[08:25:57.771324] Model = MaskedAutoencoderViT(
  (patch_embed1): PatchEmbed(
    (proj): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (act): GELU()
  )
  (patch_embed2): PatchEmbed(
    (proj): Conv2d(256, 384, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (act): GELU()
  )
  (patch_embed3): PatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (act): GELU()
  )
  (patch_embed4): Linear(in_features=768, out_features=768, bias=True)
  (stage1_output_decode): Conv2d(256, 768, kernel_size=(4, 4), stride=(4, 4))
  (stage2_output_decode): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
  (blocks1): ModuleList(
    (0): CBlock(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
      (drop_path): Identity()
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): CBlock(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
      (drop_path): Identity()
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): CBlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
)
[08:25:57.771390] base lr: 1.00e-03
[08:25:57.771396] actual lr: 2.50e-04
[08:25:57.771400] accumulate grad iterations: 1
[08:25:57.771404] effective batch size: 64
[08:25:57.772791] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.00025
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.00025
    weight_decay: 0.05
)
[08:25:57.772865] Start training for 10 epochs
[08:25:57.774048] log_dir: ./output_dir
[08:26:06.700549] Epoch: [0]  [ 0/20]  eta: 0:02:58  lr: 0.000000  loss: 1.9349 (1.9349)  time: 8.9250  data: 2.9427  max mem: 32686
[08:26:21.486867] Epoch: [0]  [19/20]  eta: 0:00:01  lr: 0.000006  loss: 1.8064 (1.8066)  time: 1.1855  data: 0.1474  max mem: 34032
[08:26:21.696593] Epoch: [0] Total time: 0:00:23 (1.1961 s / it)
[08:26:21.696712] Averaged stats: lr: 0.000006  loss: 1.8064 (1.8066)
[08:26:26.728045] log_dir: ./output_dir
[08:26:30.726185] Epoch: [1]  [ 0/20]  eta: 0:01:19  lr: 0.000006  loss: 1.4781 (1.4781)  time: 3.9962  data: 3.2145  max mem: 34032
[08:26:45.583070] Epoch: [1]  [19/20]  eta: 0:00:00  lr: 0.000012  loss: 1.4461 (1.4354)  time: 0.9426  data: 0.1611  max mem: 34032
[08:26:45.832056] Epoch: [1] Total time: 0:00:19 (0.9552 s / it)
[08:26:45.832173] Averaged stats: lr: 0.000012  loss: 1.4461 (1.4354)
[08:26:45.839654] log_dir: ./output_dir
[08:26:49.814834] Epoch: [2]  [ 0/20]  eta: 0:01:19  lr: 0.000013  loss: 1.2616 (1.2616)  time: 3.9639  data: 3.1520  max mem: 34032
[08:27:04.802293] Epoch: [2]  [19/20]  eta: 0:00:00  lr: 0.000018  loss: 1.2138 (1.2277)  time: 0.9475  data: 0.1578  max mem: 34032
[08:27:05.034234] Epoch: [2] Total time: 0:00:19 (0.9597 s / it)
[08:27:05.034373] Averaged stats: lr: 0.000018  loss: 1.2138 (1.2277)
[08:27:05.043138] log_dir: ./output_dir
[08:27:08.498144] Epoch: [3]  [ 0/20]  eta: 0:01:09  lr: 0.000019  loss: 1.1565 (1.1565)  time: 3.4502  data: 2.6421  max mem: 34032
[08:27:23.495443] Epoch: [3]  [19/20]  eta: 0:00:00  lr: 0.000025  loss: 1.0973 (1.1084)  time: 0.9223  data: 0.1324  max mem: 34032
[08:27:23.708615] Epoch: [3] Total time: 0:00:18 (0.9333 s / it)
[08:27:23.708773] Averaged stats: lr: 0.000025  loss: 1.0973 (1.1084)
[08:27:23.717868] log_dir: ./output_dir
[08:27:27.301151] Epoch: [4]  [ 0/20]  eta: 0:01:11  lr: 0.000025  loss: 1.0807 (1.0807)  time: 3.5780  data: 2.7934  max mem: 34032
[08:27:42.358292] Epoch: [4]  [19/20]  eta: 0:00:00  lr: 0.000031  loss: 1.0330 (1.0372)  time: 0.9317  data: 0.1399  max mem: 34032
[08:27:42.639145] Epoch: [4] Total time: 0:00:18 (0.9461 s / it)
[08:27:42.639267] Averaged stats: lr: 0.000031  loss: 1.0330 (1.0372)
[08:27:42.650387] log_dir: ./output_dir
[08:27:46.542287] Epoch: [5]  [ 0/20]  eta: 0:01:17  lr: 0.000031  loss: 0.9836 (0.9836)  time: 3.8878  data: 3.0775  max mem: 34032
[08:28:01.502511] Epoch: [5]  [19/20]  eta: 0:00:00  lr: 0.000037  loss: 0.9830 (0.9945)  time: 0.9423  data: 0.1541  max mem: 34032
[08:28:01.727686] Epoch: [5] Total time: 0:00:19 (0.9539 s / it)
[08:28:01.727825] Averaged stats: lr: 0.000037  loss: 0.9830 (0.9945)
[08:28:01.736712] log_dir: ./output_dir
[08:28:05.179062] Epoch: [6]  [ 0/20]  eta: 0:01:08  lr: 0.000038  loss: 0.9455 (0.9455)  time: 3.4405  data: 2.6467  max mem: 34032
[08:28:20.172406] Epoch: [6]  [19/20]  eta: 0:00:00  lr: 0.000043  loss: 0.9597 (0.9573)  time: 0.9216  data: 0.1326  max mem: 34032
[08:28:20.402879] Epoch: [6] Total time: 0:00:18 (0.9333 s / it)
[08:28:20.402993] Averaged stats: lr: 0.000043  loss: 0.9597 (0.9573)
[08:28:20.410143] log_dir: ./output_dir
[08:28:24.233612] Epoch: [7]  [ 0/20]  eta: 0:01:16  lr: 0.000044  loss: 0.9391 (0.9391)  time: 3.8219  data: 3.0287  max mem: 34032
[08:28:39.270631] Epoch: [7]  [19/20]  eta: 0:00:00  lr: 0.000050  loss: 0.9028 (0.9202)  time: 0.9429  data: 0.1517  max mem: 34032
[08:28:39.502099] Epoch: [7] Total time: 0:00:19 (0.9546 s / it)
[08:28:39.502214] Averaged stats: lr: 0.000050  loss: 0.9028 (0.9202)
[08:28:39.509714] log_dir: ./output_dir
[08:28:42.797628] Epoch: [8]  [ 0/20]  eta: 0:01:05  lr: 0.000050  loss: 0.8594 (0.8594)  time: 3.2788  data: 2.4736  max mem: 34032
[08:28:57.784458] Epoch: [8]  [19/20]  eta: 0:00:00  lr: 0.000056  loss: 0.8333 (0.8470)  time: 0.9132  data: 0.1239  max mem: 34032
[08:28:57.998282] Epoch: [8] Total time: 0:00:18 (0.9244 s / it)
[08:28:57.998426] Averaged stats: lr: 0.000056  loss: 0.8333 (0.8470)
[08:28:58.007147] log_dir: ./output_dir
[08:29:01.823440] Epoch: [9]  [ 0/20]  eta: 0:01:16  lr: 0.000056  loss: 0.8459 (0.8459)  time: 3.8144  data: 3.0200  max mem: 34032
[08:29:16.837566] Epoch: [9]  [19/20]  eta: 0:00:00  lr: 0.000062  loss: 0.8069 (0.8089)  time: 0.9414  data: 0.1513  max mem: 34032
[08:29:17.060230] Epoch: [9] Total time: 0:00:19 (0.9526 s / it)
[08:29:17.060352] Averaged stats: lr: 0.000062  loss: 0.8069 (0.8089)
[08:29:21.878019] Training time 0:03:24
