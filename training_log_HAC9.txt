[2023-02-06 17:24:52.923] [info] Requesting resources for KT AI Accelerator from the server...
[2023-02-06 17:24:53.940] [info] Initializing the worker daemon for KT AI Accelerator
[2023-02-06 17:24:54.581] [info] [1/1] Connecting to resources on the server (192.168.110.0:24171)...
[2023-02-06 17:24:54.596] [info] Establishing links to the resources...
[2023-02-06 17:24:54.638] [info] KT AI Accelerator is ready to use.
The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
Not using distributed mode
[17:24:52.861091] job dir: /nas/thuchk/repos/FastConvMAE
[17:24:52.861158] Namespace(accum_iter=1,
batch_size=64,
blr=0.001,
data_path='/nas/common_data/imagenet_tiny',
device='cuda',
dist_on_itp=False,
dist_url='env://',
distributed=False,
epochs=10,
input_size=224,
local_rank=-1,
log_dir='./output_dir',
lr=None,
mask_ratio=0.75,
min_lr=0.0,
model='fastconvmae_convvit_base_patch16',
norm_pix_loss=False,
num_workers=10,
output_dir='./output_dir',
pin_mem=True,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.05,
world_size=1)
[17:24:55.232523] Dataset ImageFolder
    Number of datapoints: 1300
    Root location: /nas/common_data/imagenet_tiny/train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
           )
[17:24:55.232814] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fe07e28eeb0>
[17:24:56.817031] Model = MaskedAutoencoderViT(
  (patch_embed1): PatchEmbed(
    (proj): Conv2d(3, 256, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (act): GELU()
  )
  (patch_embed2): PatchEmbed(
    (proj): Conv2d(256, 384, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (act): GELU()
  )
  (patch_embed3): PatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (act): GELU()
  )
  (patch_embed4): Linear(in_features=768, out_features=768, bias=True)
  (stage1_output_decode): Conv2d(256, 768, kernel_size=(4, 4), stride=(4, 4))
  (stage2_output_decode): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
  (blocks1): ModuleList(
    (0): CBlock(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
      (drop_path): Identity()
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): CBlock(
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)
      (drop_path): Identity()
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (blocks2): ModuleList(
    (0): CBlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): CBlock(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (conv1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (conv2): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
      (attn): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384)
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): CMlp(
        (fc1): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1))
        (act): GELU()
        (fc2): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1))
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (blocks3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)
  (decoder_blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
)
[17:24:56.817096] base lr: 1.00e-03
[17:24:56.817103] actual lr: 2.50e-04
[17:24:56.817108] accumulate grad iterations: 1
[17:24:56.817112] effective batch size: 64
[17:24:56.822237] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.00025
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.00025
    weight_decay: 0.05
)
[17:24:56.822325] Start training for 10 epochs
[17:24:56.824807] log_dir: ./output_dir
[17:25:01.824166] Epoch: [0]  [ 0/20]  eta: 0:01:39  lr: 0.000000  loss: 1.9349 (1.9349)  time: 4.9959  data: 0.6764  max mem: 43152
[17:25:53.713220] Epoch: [0]  [19/20]  eta: 0:00:02  lr: 0.000006  loss: 1.8734 (1.8743)  time: 2.8442  data: 0.0340  max mem: 47156
[17:25:53.768914] Epoch: [0] Total time: 0:00:56 (2.8472 s / it)
[17:25:53.774020] Averaged stats: lr: 0.000006  loss: 1.8734 (1.8743)
[17:26:05.753743] log_dir: ./output_dir
[17:26:08.980416] Epoch: [1]  [ 0/20]  eta: 0:01:04  lr: 0.000006  loss: 1.5949 (1.5949)  time: 3.2237  data: 0.6669  max mem: 47156
[17:27:00.942097] Epoch: [1]  [19/20]  eta: 0:00:02  lr: 0.000012  loss: 1.5092 (1.5174)  time: 2.7591  data: 0.0335  max mem: 47156
[17:27:01.011054] Epoch: [1] Total time: 0:00:55 (2.7629 s / it)
[17:27:01.014997] Averaged stats: lr: 0.000012  loss: 1.5092 (1.5174)
[17:27:01.057584] log_dir: ./output_dir
[17:27:04.669299] Epoch: [2]  [ 0/20]  eta: 0:01:12  lr: 0.000013  loss: 1.3331 (1.3331)  time: 3.6093  data: 0.6663  max mem: 47156
[17:27:58.306554] Epoch: [2]  [19/20]  eta: 0:00:02  lr: 0.000018  loss: 1.2968 (1.3027)  time: 2.8622  data: 0.0335  max mem: 47156
[17:27:58.372784] Epoch: [2] Total time: 0:00:57 (2.8658 s / it)
[17:27:58.377013] Averaged stats: lr: 0.000018  loss: 1.2968 (1.3027)
[17:27:58.419313] log_dir: ./output_dir
[17:28:02.075160] Epoch: [3]  [ 0/20]  eta: 0:01:13  lr: 0.000019  loss: 1.1867 (1.1867)  time: 3.6538  data: 0.7280  max mem: 47156
[17:28:55.363305] Epoch: [3]  [19/20]  eta: 0:00:02  lr: 0.000025  loss: 1.1547 (1.1728)  time: 2.8470  data: 0.0365  max mem: 47156
[17:28:55.426799] Epoch: [3] Total time: 0:00:57 (2.8504 s / it)
[17:28:55.430778] Averaged stats: lr: 0.000025  loss: 1.1547 (1.1728)
[17:28:55.474183] log_dir: ./output_dir
[17:28:58.962657] Epoch: [4]  [ 0/20]  eta: 0:01:09  lr: 0.000025  loss: 1.0982 (1.0982)  time: 3.4858  data: 0.7103  max mem: 47156
[17:29:54.414237] Epoch: [4]  [19/20]  eta: 0:00:02  lr: 0.000031  loss: 1.0859 (1.0891)  time: 2.9468  data: 0.0357  max mem: 47156
[17:29:54.479148] Epoch: [4] Total time: 0:00:59 (2.9502 s / it)
[17:29:54.482904] Averaged stats: lr: 0.000031  loss: 1.0859 (1.0891)
[17:29:54.527988] log_dir: ./output_dir
[17:29:58.082665] Epoch: [5]  [ 0/20]  eta: 0:01:11  lr: 0.000031  loss: 1.0287 (1.0287)  time: 3.5529  data: 0.6785  max mem: 47156
[17:30:52.380112] Epoch: [5]  [19/20]  eta: 0:00:02  lr: 0.000037  loss: 1.0253 (1.0300)  time: 2.8924  data: 0.0341  max mem: 47156
[17:30:52.452831] Epoch: [5] Total time: 0:00:57 (2.8962 s / it)
[17:30:52.458243] Averaged stats: lr: 0.000037  loss: 1.0253 (1.0300)
[17:30:52.502214] log_dir: ./output_dir
[17:30:55.810204] Epoch: [6]  [ 0/20]  eta: 0:01:06  lr: 0.000038  loss: 0.9871 (0.9871)  time: 3.3063  data: 0.7038  max mem: 47156
[17:31:45.869766] Epoch: [6]  [19/20]  eta: 0:00:02  lr: 0.000043  loss: 0.9674 (0.9773)  time: 2.6682  data: 0.0353  max mem: 47156
[17:31:45.942117] Epoch: [6] Total time: 0:00:53 (2.6720 s / it)
[17:31:45.947729] Averaged stats: lr: 0.000043  loss: 0.9674 (0.9773)
[17:31:45.991685] log_dir: ./output_dir
[17:31:49.514454] Epoch: [7]  [ 0/20]  eta: 0:01:10  lr: 0.000044  loss: 0.9261 (0.9261)  time: 3.5198  data: 0.6695  max mem: 47156
[17:32:42.267756] Epoch: [7]  [19/20]  eta: 0:00:02  lr: 0.000050  loss: 0.8784 (0.8864)  time: 2.8136  data: 0.0336  max mem: 47156
[17:32:42.342771] Epoch: [7] Total time: 0:00:56 (2.8175 s / it)
[17:32:42.348140] Averaged stats: lr: 0.000050  loss: 0.8784 (0.8864)
[17:32:42.392940] log_dir: ./output_dir
[17:32:45.969953] Epoch: [8]  [ 0/20]  eta: 0:01:11  lr: 0.000050  loss: 0.8267 (0.8267)  time: 3.5753  data: 0.7287  max mem: 47156
[17:33:36.539245] Epoch: [8]  [19/20]  eta: 0:00:02  lr: 0.000056  loss: 0.7944 (0.7930)  time: 2.7072  data: 0.0366  max mem: 47156
[17:33:36.619640] Epoch: [8] Total time: 0:00:54 (2.7113 s / it)
[17:33:36.624838] Averaged stats: lr: 0.000056  loss: 0.7944 (0.7930)
[17:33:36.669950] log_dir: ./output_dir
[17:33:40.038690] Epoch: [9]  [ 0/20]  eta: 0:01:07  lr: 0.000056  loss: 0.7429 (0.7429)  time: 3.3664  data: 0.7323  max mem: 47156
[17:34:31.464018] Epoch: [9]  [19/20]  eta: 0:00:02  lr: 0.000062  loss: 0.7225 (0.7287)  time: 2.7395  data: 0.0367  max mem: 47156
[17:34:31.533063] Epoch: [9] Total time: 0:00:54 (2.7432 s / it)
[17:34:31.537782] Averaged stats: lr: 0.000062  loss: 0.7225 (0.7287)
[17:34:42.437850] Training time 0:09:45
